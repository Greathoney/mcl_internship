{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.value_table = np.zeros((5, 5))\n",
    "\n",
    "    def update(self):\n",
    "        self.next_value_table = self.value_table.copy()\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                self.next_value_table[row, col] = 0\n",
    "                for action in range(4):\n",
    "                    next_row, next_col, reward = self.step((row, col), action)\n",
    "                    self.next_value_table[row, col] += 0.25 * (reward + 0.9 * self.value_table[next_row, next_col])\n",
    "        self.value_table = self.next_value_table.copy()\n",
    "\n",
    "    def step(self, state:tuple[int, int], action)->tuple[int, int, int]: # (next_row, next_col), reward\n",
    "\n",
    "        if state[0] == 0 and state[1] == 1:\n",
    "            next_row, next_col = 4, 1\n",
    "            reward = 10\n",
    "\n",
    "        elif state[0] == 0 and state[1] == 3:\n",
    "            next_row, next_col = 2, 3\n",
    "            reward = 5\n",
    "\n",
    "        else:  # 일반적인 경우\n",
    "            if action == 0:\n",
    "                next_row, next_col = state[0] - 1, state[1]\n",
    "            elif action == 1:\n",
    "                next_row, next_col = state[0] + 1, state[1]\n",
    "            elif action == 2:\n",
    "                next_row, next_col = state[0], state[1] - 1\n",
    "            else: # elif action == 3:\n",
    "                next_row, next_col = state[0], state[1] + 1\n",
    "\n",
    "            # 그리드 밖으로 나가는 경우\n",
    "            if next_row < 0 or next_row >= 5 or next_col < 0 or next_col >= 5:\n",
    "                reward = -1\n",
    "                next_row, next_col = state[0], state[1]\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        return next_row, next_col, reward\n",
    "    \n",
    "    def get_poilcy(self):\n",
    "        dir_table = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
    "        direction_list = []\n",
    "        for row in range(5):\n",
    "            direction_list_row = []\n",
    "            for col in range(5):\n",
    "                if row == 0 and col == 1 or row == 0 and col == 3:\n",
    "                    direction_list_row.append(\"*\")\n",
    "                    continue\n",
    "                action_value = []\n",
    "                for action in range(4):\n",
    "                    if action == 0:\n",
    "                        try:\n",
    "                            action_value.append(self.value_table[row-1, col])\n",
    "                        except IndexError:\n",
    "                            action_value.append(-999)\n",
    "                    elif action == 1:\n",
    "                        try:\n",
    "                            action_value.append(self.value_table[row+1, col])\n",
    "                        except IndexError:\n",
    "                            action_value.append(-999)\n",
    "                    elif action == 2:\n",
    "                        try:\n",
    "                            action_value.append(self.value_table[row, col-1])\n",
    "                        except IndexError:\n",
    "                            action_value.append(-999)\n",
    "                    else: # elif action == 3:\n",
    "                        try:\n",
    "                            action_value.append(self.value_table[row, col+1])\n",
    "                        except IndexError:\n",
    "                            action_value.append(-999)\n",
    "\n",
    "                direction_list_row.append(dir_table[np.argmax(action_value)])\n",
    "            direction_list.append(direction_list_row)\n",
    "        return direction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 상태 가치 함수\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "무작위 행동으로 수렴된 상태 가치 함수 (100번 반복)\n",
      "[[ 3.30899932  8.78929484  4.42762216  5.32237057  1.49218174]\n",
      " [ 1.52159105  2.99232084  2.25014293  1.90757468  0.54740569]\n",
      " [ 0.05082547  0.73817357  0.67311624  0.35818919 -0.40313816]\n",
      " [-0.97358932 -0.43549245 -0.35487929 -0.58560211 -1.1830721 ]\n",
      " [-1.85769757 -1.34522828 -1.22926428 -1.42291517 -1.97517607]]\n",
      "\n",
      "상태 가치 함수로 계산한 정책\n",
      "[['→' '*' '←' '*' '←']\n",
      " ['↑' '↑' '↑' '↑' '←']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']\n",
      " ['↑' '↑' '↑' '↑' '↑']]\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "print(\"초기 상태 가치 함수\")\n",
    "print(env.value_table)\n",
    "\n",
    "for i in range(100):\n",
    "    env.update()\n",
    "\n",
    "print(\"\\n무작위 행동으로 수렴된 상태 가치 함수 (100번 반복)\")\n",
    "print(env.value_table)\n",
    "\n",
    "print(\"\\n상태 가치 함수로 계산한 정책\")\n",
    "print(np.array(env.get_poilcy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2:\n",
    "    def __init__(self):\n",
    "        self.q_table = np.zeros((5, 5, 4))\n",
    "\n",
    "    def update(self):\n",
    "        self.next_q_table = self.q_table.copy()\n",
    "        for row in range(5):\n",
    "            for col in range(5):\n",
    "                for action in range(4):\n",
    "                    (next_row, next_col), reward = self.get_next_state(row, col, action)\n",
    "                    self.next_q_table[row, col, action] = (reward + 0.9 * np.max(self.q_table[next_row, next_col, :]))\n",
    "        self.q_table = self.next_q_table.copy()\n",
    "\n",
    "    def get_next_state(self, row, col, action):\n",
    "\n",
    "        if row == 0 and col == 1:\n",
    "            next_row, next_col = 4, 1\n",
    "            reward = 10\n",
    "\n",
    "        elif row == 0 and col == 3:\n",
    "            next_row, next_col = 2, 3\n",
    "            reward = 5\n",
    "\n",
    "        else:  # 일반적인 경우\n",
    "            if action == 0:\n",
    "                next_row, next_col = row - 1, col\n",
    "            elif action == 1:\n",
    "                next_row, next_col = row + 1, col\n",
    "            elif action == 2:\n",
    "                next_row, next_col = row, col - 1\n",
    "            else: # elif action == 3:\n",
    "                next_row, next_col = row, col + 1\n",
    "\n",
    "            # 그리드 밖으로 나가는 경우\n",
    "            if next_row < 0 or next_row >= 5 or next_col < 0 or next_col >= 5:\n",
    "                reward = -1\n",
    "                next_row, next_col = row, col\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "        return (next_row, next_col), reward\n",
    "    \n",
    "    def get_poilcy(self):\n",
    "        dir_dict = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
    "        direction_list = []\n",
    "        for row in range(5):\n",
    "            direction_list_row = []\n",
    "            for col in range(5):\n",
    "                if row == 0 and col == 1 or row == 0 and col == 3:\n",
    "                    direction_list_row.append(\"* \")\n",
    "                    continue\n",
    "                action_value = \"\"\n",
    "                \n",
    "                # 최대값을 가지는 행동의 모든 인덱스를 가져옴\n",
    "                max_idx_list = np.argwhere(self.q_table[row, col, :] == np.max(self.q_table[row, col, :])).flatten().tolist()\n",
    "\n",
    "                for i in max_idx_list:\n",
    "                    action_value += dir_dict[i]\n",
    "                for i in range(2-len(max_idx_list)):\n",
    "                    action_value += \" \"\n",
    "\n",
    "                direction_list_row.append(action_value)\n",
    "\n",
    "            direction_list.append(direction_list_row)\n",
    "        return direction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 상태 가치 함수\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "최적 행동 가치 함수로 계산한 최적 상태 가치 함수  (100번 반복)\n",
      "[[21.97690153 24.41877948 21.97690153 19.41877948 17.47690153]\n",
      " [19.77921138 21.97690153 19.77921138 17.80129024 16.02116122]\n",
      " [17.80129024 19.77921138 17.80129024 16.02116122 14.41877948]\n",
      " [16.02116122 17.80129024 16.02116122 14.41877948 12.97690153]\n",
      " [14.41877948 16.02116122 14.41877948 12.97690153 11.67921138]]\n",
      "\n",
      "최적 행동 가치 함수로 계산한 정책\n",
      "[['→ ' '* ' '← ' '* ' '← ']\n",
      " ['↑→' '↑ ' '↑←' '← ' '← ']\n",
      " ['↑→' '↑ ' '↑←' '↑←' '↑←']\n",
      " ['↑→' '↑ ' '↑←' '↑←' '↑←']\n",
      " ['↑→' '↑ ' '↑←' '↑←' '↑←']]\n"
     ]
    }
   ],
   "source": [
    "env = Environment2()\n",
    "print(\"초기 상태 가치 함수\")\n",
    "print(np.max(env.q_table, axis=2))\n",
    "\n",
    "for i in range(100):\n",
    "    env.update()\n",
    "\n",
    "print(\"\\n최적 행동 가치 함수로 계산한 최적 상태 가치 함수  (100번 반복)\")\n",
    "print(np.max(env.q_table, axis=2))\n",
    "\n",
    "print(\"\\n최적 행동 가치 함수로 계산한 정책\")\n",
    "print(np.array(env.get_poilcy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frequency",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
