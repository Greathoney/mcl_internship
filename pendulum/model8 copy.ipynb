{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import Model\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.dense1 = Dense(128, activation=\"relu\", input_dim=state_size)\n",
    "        self.dense2 = Dense(128, activation=\"relu\")\n",
    "        self.dense3 = Dense(64, activation=\"relu\")\n",
    "        self.dense4 = Dense(32, activation=\"relu\")\n",
    "        self.dense5 = Dense(action_size, activation=\"softmax\")\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.dense4(x)\n",
    "        x = self.dense5(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state):\n",
    "        self.replay_memory.append((state, action, reward, next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model: DQN):\n",
    "    # 리플레이 버퍼 크기가 작으면 업데이트하지 않음\n",
    "    if len(model.replay_memory) < 1000:\n",
    "        return\n",
    "    \n",
    "    # 너무 많으면 리플레이 버퍼 pop\n",
    "    if len(model.replay_memory) > 30000:\n",
    "        del model.replay_memory[0]\n",
    "\n",
    "    # # 메모리에서 랜덤 샘플링\n",
    "    sample_size = 500\n",
    "    samples = random.sample(model.replay_memory, sample_size)\n",
    "\n",
    "    # 분할\n",
    "    states, actions, rewards, next_states = zip(*samples)\n",
    "\n",
    "    # numpy 배열로 변환\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_states = np.array(next_states)\n",
    "    \n",
    "    # 모델 예측과 타겟 값 계산\n",
    "    targets = model.call(states).numpy()  # type: ignore\n",
    "    next_q_values = model.call(next_states).numpy() # type: ignore\n",
    "\n",
    "    targets[np.arange(len(samples)), actions] = rewards + 0.99 * np.max(next_q_values, axis=1)\n",
    "\n",
    "    # 모델 업데이트\n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = model.call(states)\n",
    "        loss = tf.keras.losses.mean_squared_error(targets, q_values)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "def draw_qvalue(model, episode):\n",
    "    thetas = np.arange(-np.pi, np.pi, 0.1)\n",
    "    velocities = np.arange(-4, 4, 0.1)\n",
    "\n",
    "    states = np.array([[np.cos(theta), np.sin(theta), velocity] for theta in thetas for velocity in velocities])\n",
    "    z = model.call(states).numpy()\n",
    "    z_np = np.array(z).reshape((len(thetas), len(velocities), 9))\n",
    "\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    fig.suptitle(\"Episode: {}\".format(episode))\n",
    "    for i in range(9):\n",
    "        ax[i//5][i%5].title.set_text(\"Action: {}\".format(i))\n",
    "        ax[i//5][i%5].imshow(z_np[:,:,i], cmap='hot', interpolation='nearest', clim=(0, 1))\n",
    "        ax[i//5][i%5].set_xticks(np.arange(0, 81, 10))\n",
    "        ax[i//5][i%5].set_yticks(np.arange(0, 64, 10))\n",
    "        ax[i//5][i%5].set_xticklabels(np.arange(-4, 5, 1))\n",
    "        ax[i//5][i%5].set_yticklabels(np.round(np.arange(-np.pi, np.pi+0.1, 2*np.pi/6),1))\n",
    "        ax[i//5][i%5].set_xlabel(\"Velocity\")\n",
    "        ax[i//5][i%5].set_ylabel(\"Theta\")\n",
    "        ax[i//5][i%5].invert_yaxis()  # y축 변경\n",
    "    ax[1][4].axis('off')  # 마지막 subplot은 빈 공간\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model8_image/episode_{}.png\".format(episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity = 10\n",
    "# env = gym.make(\"Pendulum-v1\", g=9.81, render_mode=\"human\")\n",
    "env = gym.make(\"Pendulum-v1\", g=gravity, render_mode=\"human\")\n",
    "model = DQN(3, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n",
      "A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(\u001b[39m9\u001b[39m, p\u001b[39m=\u001b[39maction)\n\u001b[0;32m     21\u001b[0m \u001b[39m# 행동 실행\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m next_state, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep((action\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m,))\n\u001b[0;32m     23\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m next_state[\u001b[39m0\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.985\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mabs\u001b[39m(next_state[\u001b[39m2\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m0.2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\honey\\anaconda3\\envs\\frequency\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\honey\\anaconda3\\envs\\frequency\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\honey\\anaconda3\\envs\\frequency\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\honey\\anaconda3\\envs\\frequency\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:142\u001b[0m, in \u001b[0;36mPendulumEnv.step\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([newth, newthdot])\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs(), \u001b[39m-\u001b[39mcosts, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\honey\\anaconda3\\envs\\frequency\\lib\\site-packages\\gymnasium\\envs\\classic_control\\pendulum.py:257\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    256\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 257\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    258\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    260\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# mode == \"rgb_array\":\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(1000):\n",
    "    if episode % 10 == 0:\n",
    "        env = gym.make(\"Pendulum-v1\", g=gravity, render_mode=\"human\")\n",
    "    else:\n",
    "        env = gym.make(\"Pendulum-v1\", g=gravity)\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    step = 0\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    nice_state = False\n",
    "\n",
    "    while not terminated and step < 500:\n",
    "\n",
    "        # 모델로 행동 예측\n",
    "        action = model.call(np.array([state])).numpy()[0]  # type: ignore\n",
    "        action = np.random.choice(9, p=action)\n",
    "\n",
    "        # 행동 실행\n",
    "        next_state, reward, terminated, truncated, info = env.step((action/2-2,))\n",
    "        reward = 0\n",
    "\n",
    "        if next_state[0] > 0.985 and abs(next_state[2]) < 0.2:\n",
    "            nice_state = True\n",
    "            reward += 50  # type: ignore\n",
    "        elif nice_state:\n",
    "            nice_state = False\n",
    "            reward -= 50  # type: ignore\n",
    "\n",
    "        if next_state[0] > 0.9:\n",
    "            if next_state[0] < state[0]:\n",
    "                if abs(next_state[2]) > abs(state[2]):\n",
    "                    reward -= 10  # type: ignore\n",
    "            if next_state[0] > state[0]:\n",
    "                reward += 10  # type: ignore\n",
    "\n",
    "        if next_state[0] < -0.5:\n",
    "            if next_state[2] * (action/2 - 2) > 0:\n",
    "                reward += 1\n",
    "            else:\n",
    "                reward -= 1\n",
    "\n",
    "\n",
    "        # 리플레이 버퍼에 기억\n",
    "        model.remember(state, action, reward, next_state)\n",
    "        \n",
    "        # 모델 업데이트\n",
    "        if step % 50 == 0:\n",
    "            update_model(model)\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # if episode % 10 == 0:\n",
    "    #     draw_qvalue(model, episode)\n",
    "    \n",
    "    print(\"Episode: {}, Steps: {}, Score: {:.2f}, Last Score: {:.2f}\".format(episode, step, sum(rewards) / len(rewards), sum(rewards[-10:])/10))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model8', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
